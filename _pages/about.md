---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
classes: "about-page"
---

I'm a Ph.D candidate at <a href="https://mli.kaist.ac.kr/" target="_blank" style="color: black;">MLILAB</a> at <a href="
https://www.kaist.ac.kr/en/" target="_blank" style="color: black;">KAIST</a> in Korea, advised by <a href="https://scholar.google.com/citations?user=UWO1mloAAAAJ&hl=ko&oi=ao" target="_blank" style="color: black;">Prof. Eunho Yang</a>.

## Research Interests

My current research interests lie in building **self-improving agents** (closely working with <a href="https://www.linkedin.com/in/ninareh-mehrabi-99851488/" target="_blank" style="color: black;"> Ninareh Mehrabi </a>  and  <a href="https://www.linkedin.com/in/sina-shaham-17b234189/" target="_blank" style="color: black;"> Sina Shaham</a>) and developing **reliable Visual Chain-of-Thought (CoT) reasoning** for vision–language models. 

Beyond these, I have explored a broad range of topics in **Reliable AI**, including model robustness spanning both 2D and 3D vision as well as domain-agnostic methods, **jailbreaking in MLLMs**, **Visual Autoregressive Models**, and **image editing**.

<!---
<hr>
<span style="color: #1A73E8; font-weight: bold;">Reliable AI </span>  My research, in general, has focused on **model robustness**, including both general methods and specialized approaches for 2D & 3D vision, graph, and vision-language models. As part of this direction, I have recently been developing robust fine-tuning methods for vision-language models to ensure reliable deployment in real-world applications.

<span style="color: #1A73E8; font-weight: bold;">Agents & MLLMs </span>  My current research focuses on **self-improving agents for generalization** (closely working with <a href="https://scholar.google.com/citations?user=1R3XgHQAAAAJ&hl=en" target="_blank" style="color: black;"> Ninareh Mehrabi </a> <img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Amazon_logo.svg" alt="Amazon" width="50" style="vertical-align: middle; position: relative; top: 2px;"> and  <a href="https://scholar.google.com.au/citations?user=WnWN4NkAAAAJ&hl=en" target="_blank" style="color: black;"> Sina Shaham</a> <img src="https://logodownload.org/wp-content/uploads/2021/10/meta-logo-1.png" alt="Meta" width="50" style="vertical-align: middle; position: relative; top: -2px;">) and **addressing hallucination in MLLMs through visual reasoning**.
-->

<!---
My recent research interests focus on
        <span style="color: #1A73E8; font-weight: bold;">(i) self-improving large multi-modal agents for generalization</span>  (closely working with <a href="https://scholar.google.com/citations?user=1R3XgHQAAAAJ&hl=en" target="_blank" style="color: black;"> Ninareh Mehrabi </a> <img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Amazon_logo.svg" alt="Amazon" width="50" style="vertical-align: middle; position: relative; top: 2px;"> and  <a href="https://scholar.google.com.au/citations?user=WnWN4NkAAAAJ&hl=en" target="_blank" style="color: black;"> Sina Shaham</a> <img src="https://logodownload.org/wp-content/uploads/2021/10/meta-logo-1.png" alt="Meta" width="50" style="vertical-align: middle; position: relative; top: -2px;">)
        and 
        <span style="color: #1A73E8; font-weight: bold;">(ii) addressing hallucination in multi-modal large language models </span>. 
        I am also interested in developing 
        <span style="color: #1A73E8; font-weight: bold;">(iii) robust fine-tuning methods for vision-language models</span> 
        to ensure reliable deployment in real-world applications.
Previously, my work centered on understanding and improving model robustness across diverse modalities, including 2D and 3D vision, graph, and multi-modal models. 
-->

<!---
My interests include, but are not limited to, ***understanding and enhancing model robustness*** across diverse modalities such as 2D & 3D vision, vision-language, and multi-modal models. 

Recently, my research has focused on ***robust learning/fine-tuning methods for large vision-language and multi-modal autoregressive models***, aimed at improving adaptability and resilience to diverse data distributions and task variations, thereby enhancing performance consistency in complex, real-world scenarios. 
-->

Please feel free to contact me if you are interested in potential collaborations! &nbsp; <a href="https://www.linkedin.com/in/yeonsung-jung-a50015213/" target="_blank" style="color: #1E90FF">
    <img src="https://cdn-icons-png.flaticon.com/512/174/174857.png" alt="LinkedIn" width="15" height="15"> LinkedIn </a>



<!---
My research interest falls into enhancing the understanding of unstructured/video data modalities through the guidance of large language models. With these goals in mind, my recent focus has been on linking diverse modalities into the core of large language model **through the lens of graph-structured knowledge**, *e.g.* object graphs (3D vision), knowledge graphs (natural language), and scene graphs (video). In this endeavor, I work on building algorithms that leverage relational information of data therein, **revisiting real-world problems within a graph-based framework to provide a structured understanding of complex data modalities** in large language models.
- Multimodal Large language models: Generation and Comprehension
- Compositional Generalization (Object-centric Learning)
- Graph-driven Modal Understanding
-->


<!---**Learning on 3D Vision**\\
My primary research interest in 3D vision falls into two branches following: 1) **Cross-modal 3D understanding**. It aims to harness the power of auxiliary data modalities for an in-depth comprehension of complex 3D data. Currently, I'm working on open-vocabulary 3D scene segmentation with object-relational graphs leveraging recent language foundation models' capabilities. 2) **Sim-to-real adaptation for 3D data**. My recent research efforts have been dedicated to narrowing the domain gap between synthetic and real-world 3D data. Ranging from developing adaptation strategies to curating 3D photorealistic datasets, my recent objective is to facilitate successful sim-to-real transfer across a broad range of 3D vision tasks.
-->

## Preprints
<hr>
- Co-Evolving Agents: Learning from Failures as Hard Negatives \\
<span style="font-size: 90%;"> **Yeonsung Jung**, Trilok Padhi, Sina Shaham, Dipika Khullar, Joonhyun Jeong, Ninareh Mehrabi†, Eunho Yang† </span> <br>

- Web Agents Are Still Greedy: Progress-Aware Action Generation and Selection via Meta-Plan \\
<span style="font-size: 90%;"> Joonhyun Jeong, Gilhyun Nam, **Yeonsung Jung**, Eunho Yang </span> <br>

- MeZO-A<sup>3</sup>dam: Memory-efficient Zeroth-order Adam with Adaptivity Adjustments for Fine-tuning LLMs \\
<span style="font-size: 90%;"> Sihwan Park\*, Jihun Yun\*, Sung-Yub Kim, June Yong Yang, **Yeonsung Jung**, Souvik Kundu, Kyungsu Kim, Eunho Yang </span> <br>

- 3D Scene Decomposition Under Occlusion via Multi-View-Aware Inpainting \\
<span style="font-size: 90%;"> Heecheol Yun, **Yeonsung Jung**, Eunho Yang </span> <br>

## Conference Publications
<hr>
- Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing <a href="https://arxiv.org/abs/2504.13490" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> Joowon Kim, Ziseok Lee, Donghyeon Cho, Sanghyun Jo, **Yeonsung Jung**, Kyungsu Kim, Eunho Yang </span>\\
<span style="color:darkred">**ICCV**</span> 2025

- Preserve or Modify? Context-Aware Evaluation for Balancing Preservation and Modification in Text-Guided Image Editing <a href="https://arxiv.org/abs/2410.11374" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> Yoonjeon Kim\*, Soohyun Ryu\*, **Yeonsung Jung**, Hyunkoo Lee, Joowon Kim, June Yong Yang, Jaeryong Hwang, Eunho Yang </span>\\
<span style="color:darkred">**CVPR**</span> 2025

- Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy <a href="https://arxiv.org/abs/2503.20823" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> Joonhyun Jeong, Seyun Bae, **Yeonsung Jung**, Jaeryong Hwang, Eunho Yang </span>\\
<span style="color:darkred">**CVPR**</span> 2025

- LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding <a href="https://arxiv.org/abs/2410.03355" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> Doohyuk Jang\*, Sihwan Park\*, June Yong Yang, **Yeonsung Jung**, Jihun Yun, Souvik Kundu, Sung-Yub Kim†, Eunho Yang† </span>\\
<span style="color:darkred">**ICLR**</span> 2025

- A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective <a href="https://arxiv.org/abs/2411.00360" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> **Yeonsung Jung\***, Jaeyun Song\*, June Yong Yang, Jin-Hwa Kim, Sung-Yub Kim, Eunho Yang </span>\\
<span style="color:darkred">**NeurIPS**</span> 2024

- PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency <a href="https://proceedings.mlr.press/v235/jung24b.html" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> **Yeonsung Jung**, Heecheol Yun, Joonhyung Park, Jin-Hwa Kim†, Eunho Yang† </span>\\
<span style="color:darkred">**ICML**</span> 2024

- Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation <a href="https://proceedings.mlr.press/v202/jung23b.html" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> **Yeonsung Jung**, Hajin Shim, June Yong Yang, Eunho Yang </span>\\
<span style="color:darkred">**ICML**</span> 2023

- Scalable Anti-TrustRank with Qualified Site-level Seeds for Link-based Web Spam Detection <a href="https://dl.acm.org/doi/pdf/10.1145/3366424.3385773" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> Joyce Jiyoung Whang, **Yeonsung Jung**, Seonggoo Kang, Dongho Yoo, Inderjit S. Dhillon </span>\\
<span style="color:darkred">**The Web Conf. Workshop**</span> on CyberSafety: Computational Methods in Online Misbehavior 2020

- Fast Asynchronous Anti-TrustRank for Web Spam Detection <a href="https://snap.stanford.edu/mis2/files/MIS2_paper_24.pdf" target="_blank" style="color: #1E90FF">[paper]</a> \\
<span style="font-size: 90%;"> Joyce Jiyoung Whang, **Yeonsung Jung**, Inderjit S. Dhillon, Seonggoo Kang, Jungmin Lee </span>\\
<span style="color:darkred">**WSDM Workshop**</span> on MIS2: Misinformation and Misbehavior Mining on the Web 2018

## Work Experiences
<hr>
- Research Intern, **<a href="https://naver-career.gitbook.io/en/publications/all" target="_blank" style="color: black;"> NAVER </a>**, Seongnam, South Korea. (July 2019 - Sept. 2019)
  - Improve searching performance through click graph neural networks.

<!---
## Projects
- Sub-task generation based point/regional Out-Of-Distribution detection, **Samsung Electronics**, <font size="3">Sep. 2020 - Sep. 2025</font>
- Predicting graph properties with few labels using Graph Neural Networks, **Samsung Electronics**, <font size="3">Sep. 2020 - Sep. 2025</font>
- Machine learning model for the prediction of Hypoxaemia during Endoscopic Retrograde Cholangiopancreatography, **Yonsei Severance Hospital**, <font size="3">Mar. 2020 - Jun. 2020</font>
    - Published in [Yonsei Medical Journal](https://ymj.kr/DOIx.php?id=10.3349/ymj.2022.0381)
-->

## Acamdeic Services
<hr>
- Conference Reviewer
    - Neural Information Processing Systems (NeurIPS)
    - International Conference on Machine Learning (ICML)
    - International Conference on Learning Representations (ICLR)
    - Computer Vision and Pattern Recognition (CVPR)
    - International Conference on Computer Vision (ICCV)
    - Association for the Advancement of Artificial Intelligence (AAAI)
    - Artificial Intelligence and Statistics (AISTATS)
    - NACCL Workshop on TrustNLP
      
<!---
  - Computer Vision and Pattern Recognition (CVPR)
  - AAAI Conference on Artificial Intelligence (AAAI)
  - International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
  - Learning on Graphs (LoG)
- Journal Reviewer
  - Transactions on Neural Networks and Learning Systems (TNNLS)
-->

## Lectures
<hr>
- <img src="images/samsung.png" alt="SAMSUNG DS" style="height:1em; vertical-align:middle;"/> &nbsp; **SAMSUNG DS** _(2020–2022)_
- <img src="images/lge.png" alt="LG Electronics" style="height:1em; vertical-align:middle;"/> &nbsp; **LG Electronics** _(2022)_
- <img src="images/hd.png" alt="HD Hyundai" style="height:1em; vertical-align:middle;"/> &nbsp; **HD Hyundai** _(2023)_
- <img src="images/innotek.png" alt="LG Innotek" style="height:1em; vertical-align:middle;"/> &nbsp; **LG Innotek** _(2022–2024)_
- <img src="images/lgchem.png" alt="LG Chem" style="height:1em; vertical-align:middle;"/> &nbsp; **LG Chem** _(2022)_
- <img src="images/dsme.png" alt="DSME" style="height:1em; vertical-align:middle;"/> &nbsp; **DSME** _(2022–2023)_

